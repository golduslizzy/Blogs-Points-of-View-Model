{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7789500,"sourceType":"datasetVersion","datasetId":4559386}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing necessary libraries\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load train and test datasets\ntrain_essays = pd.read_excel('/kaggle/input/povdetecting/Blog POV Dataset Finalllllll.xlsx')\n\n\n# Preprocessing the data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_data(texts):\n    return tokenizer(\n        texts,\n        padding='max_length',\n        truncation=True,\n        max_length=512,\n        return_tensors='pt'\n    )\nclass EssaysDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        # Ensure labels are a list of integers\n        self.labels = labels if isinstance(labels, list) else labels.tolist()\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].detach().clone() for key, val in self.encodings.items()}\n        # Convert integer label to a tensor of dtype long\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Preprocess and split the train dataset\nX_train, X_val, y_train, y_val = train_test_split(train_essays['Text'],train_essays['POV'], test_size=0.2)\ntrain_encodings = preprocess_data(X_train.tolist())\nval_encodings = preprocess_data(X_val.tolist())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-08T14:12:28.775747Z","iopub.execute_input":"2024-03-08T14:12:28.776310Z","iopub.status.idle":"2024-03-08T14:12:29.700554Z","shell.execute_reply.started":"2024-03-08T14:12:28.776277Z","shell.execute_reply":"2024-03-08T14:12:29.698581Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#custom dataset \ntrain_dataset = EssaysDataset(train_encodings,y_train.tolist())\nval_dataset = EssaysDataset(val_encodings,y_val.tolist())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-08T10:58:18.395405Z","iopub.execute_input":"2024-03-08T10:58:18.395944Z","iopub.status.idle":"2024-03-08T10:58:18.401492Z","shell.execute_reply.started":"2024-03-08T10:58:18.395895Z","shell.execute_reply":"2024-03-08T10:58:18.400512Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=3)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=10,\n    per_device_train_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T12:13:30.074765Z","iopub.execute_input":"2024-03-08T12:13:30.075767Z","iopub.status.idle":"2024-03-08T14:12:25.820790Z","shell.execute_reply.started":"2024-03-08T12:13:30.075624Z","shell.execute_reply":"2024-03-08T14:12:25.819339Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 1:57:57, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.122700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.126000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.043300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.991900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.909000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.843900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.709100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.598500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.481800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.406400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.312600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.202600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.164000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.110700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=150, training_loss=0.6080288827419281, metrics={'train_runtime': 7130.7407, 'train_samples_per_second': 0.337, 'train_steps_per_second': 0.021, 'total_flos': 631472202547200.0, 'train_loss': 0.6080288827419281, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T14:16:35.701502Z","iopub.execute_input":"2024-03-08T14:16:35.702406Z","iopub.status.idle":"2024-03-08T14:17:24.508191Z","shell.execute_reply.started":"2024-03-08T14:16:35.702363Z","shell.execute_reply":"2024-03-08T14:17:24.506780Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.313424289226532,\n 'eval_runtime': 48.7889,\n 'eval_samples_per_second': 1.23,\n 'eval_steps_per_second': 0.164,\n 'epoch': 10.0}"},"metadata":{}}]},{"cell_type":"code","source":"Input_blog=\"Bank managers looking to understand how Blockchain works in the banking industry can benefit greatly from incorporating this technology into their operations. Blockchain technology provides transaction immutability, transparency, and provenance, eliminating the need for trust enforcers and increasing transparency between market participants. By storing immutable records of ownership and enabling secure transfers of assets among distrusting parties, Blockchain enhances trust, transparency, and efficiency in the banking sector. This technology revolutionizes how money and transactions are managed, offering a more secure and cost effective way of conducting financial transactions.\"\n","metadata":{"execution":{"iopub.status.busy":"2024-03-08T14:27:01.724931Z","iopub.execute_input":"2024-03-08T14:27:01.725326Z","iopub.status.idle":"2024-03-08T14:27:01.734094Z","shell.execute_reply.started":"2024-03-08T14:27:01.725295Z","shell.execute_reply":"2024-03-08T14:27:01.732700Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Preprocess the raw text input for prediction\ntest_encodings = preprocess_data([Input_blog])\n\n# Create a dummy label for the single input\ndummy_label = [2]  \n\n# Create a Dataset object for the single input\ntest_dataset = EssaysDataset(test_encodings, dummy_label)\n\n# Predict using the trained model\npredictions = trainer.predict(test_dataset)\n\n# Convert predictions to binary labels\npred_labels = predictions.predictions.argmax(axis=-1)\n\n# Output the prediction\nprint(\"Predicted label:\", pred_labels.item())","metadata":{"execution":{"iopub.status.busy":"2024-03-08T14:27:04.828461Z","iopub.execute_input":"2024-03-08T14:27:04.829034Z","iopub.status.idle":"2024-03-08T14:27:05.741161Z","shell.execute_reply.started":"2024-03-08T14:27:04.828984Z","shell.execute_reply":"2024-03-08T14:27:05.740042Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Predicted label: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"if pred_labels ==0:\n    print(\"First person perspective\")\nelif pred_labels == 1:\n    print(\"Second person perspective\")\nelse:\n    print (\"third person perspective\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T15:08:05.883117Z","iopub.execute_input":"2024-03-08T15:08:05.883591Z","iopub.status.idle":"2024-03-08T15:08:05.894333Z","shell.execute_reply.started":"2024-03-08T15:08:05.883554Z","shell.execute_reply":"2024-03-08T15:08:05.892987Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"third person perspective\n","output_type":"stream"}]},{"cell_type":"code","source":"#To save the model\ntrainer.save_model(\"./model\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T14:32:20.731904Z","iopub.execute_input":"2024-03-08T14:32:20.733208Z","iopub.status.idle":"2024-03-08T14:32:22.042862Z","shell.execute_reply.started":"2024-03-08T14:32:20.733151Z","shell.execute_reply":"2024-03-08T14:32:22.041811Z"},"trusted":true},"execution_count":27,"outputs":[]}]}